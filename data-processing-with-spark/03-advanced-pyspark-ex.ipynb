{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70644fe-01fa-45ca-81f0-2c64afcb3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/19 13:15:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark aggregation functions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2512b1ab-3224-443b-aaca-208e131ee584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- last_scraped: date (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: date (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: date (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- availability_eoy: integer (nullable = true)\n",
      " |-- number_of_reviews_ly: integer (nullable = true)\n",
      " |-- estimated_occupancy_l365d: integer (nullable = true)\n",
      " |-- estimated_revenue_l365d: integer (nullable = true)\n",
      " |-- first_review: date (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "listings = spark.read.csv(\"./data/listings.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\", \n",
    "    quote='\"',\n",
    "    escape='\"', \n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\" \n",
    ")\n",
    "listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1228208-30f4-4bf2-988c-1b064571960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "reviews = spark.read.csv(\"./data/reviews.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1e198f-37f4-4cbf-8b55-3db1f3c32c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------+\n",
      "|budget_type|num_of_budget_type_category|\n",
      "+-----------+---------------------------+\n",
      "|  Mid-range|                      28614|\n",
      "|     Budget|                       6612|\n",
      "|     Luxury|                      27458|\n",
      "+-----------+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. For each listing compute string category depending on its price, and add it as a new column.\n",
    "# A category is defined in the following way:\n",
    "#\n",
    "# * price < 50 -> \"Budget\"\n",
    "# * 50 <= price < 150 -> \"Mid-range\"\n",
    "# * price >= 150 -> \"Luxury\"\n",
    "# \n",
    "# Only include listings where the price is not null.\n",
    "# Count the number of listings in each category\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "listings = listings.withColumn('price_numeric', regexp_replace('price', '[$,]', '').cast('float'))\n",
    "\n",
    "# TODO: Implement a UDF\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "def classifyBudget(num: float):\n",
    "    if num < 50:\n",
    "        return \"Budget\"\n",
    "    elif num >= 50 and num <= 150:\n",
    "        return \"Mid-range\"\n",
    "    else:\n",
    "        return \"Luxury\"\n",
    "            \n",
    "classifyBudget_udf = F.udf(\n",
    "            classifyBudget,\n",
    "            StringType()\n",
    "            )\n",
    "# TODO: Apply the UDF to create a new DataFrame\n",
    "new_df = listings.withColumn(\n",
    "    \"budget_type\",\n",
    "    classifyBudget_udf('price_numeric')\n",
    ")\n",
    "# new_df.select(\"budget_type\", \"price_numeric\").limit(10).show()\n",
    "new_df\\\n",
    "    .filter(new_df.price.isNotNull())\\\n",
    "    .groupBy(\"budget_type\")\\\n",
    "    .agg(\n",
    "        F.count(\"budget_type\").alias(\"num_of_budget_type_category\")\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a133d687-46cc-4f01-a163-3e684d43d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|         listing_id|                name|sentiment_avg|\n",
      "+-------------------+--------------------+-------------+\n",
      "|           23872786|Beautiful house p...|          9.0|\n",
      "|           40386257|Beautifully Conte...|          9.0|\n",
      "|           37015443|Modern family hom...|          8.0|\n",
      "| 662470859745906578|Trent View a holi...|          8.0|\n",
      "|           34277415|Gorgeous family h...|          8.0|\n",
      "|1169478573487073390|Spacious Victoria...|          7.0|\n",
      "|1380936421100195787|Peaceful cosy Eal...|          7.0|\n",
      "|           13535637|large loft room w...|          7.0|\n",
      "|           38199388|Comfortable Famil...|          7.0|\n",
      "|            8630729|Central and Cozy ...|          7.0|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. In this task you will need to compute a santiment score per review, and then an average sentiment score per listing.\n",
    "# A santiment score indicates how \"positive\" or \"negative\" a review is. The higher the score the more positive it is, and vice-versa.\n",
    "#\n",
    "# To compute a sentiment score per review compute the number of positive words in a review and subtract the number of negative\n",
    "# words in the same review (the list of words is already provided)\n",
    "#\n",
    "# To complete this task, compute a DataFrame that contains the following fields:\n",
    "# * name - the name of a listing\n",
    "# * average_sentiment - average sentiment of reviews computed using the algorithm described above\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Lists of positive and negative words\n",
    "positive_words = {'good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'pleasant', 'lovely', 'nice', 'enjoyed'}\n",
    "negative_words = {'bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'hate', 'unpleasant', 'dirty', 'noisy'}\n",
    "\n",
    "def remove_punc(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    res = []\n",
    "    for x in s:\n",
    "        if x.isalpha() or x.isspace():\n",
    "            res.append(x)\n",
    "    return \"\".join(res)\n",
    "    \n",
    "# TODO: Implement the UDF\n",
    "def sentiment_score(comment: str) -> float:\n",
    "    if comment is None:\n",
    "        return 0.0\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    cleaned_comment = remove_punc(comment.lower())\n",
    "    for word in cleaned_comment.split():\n",
    "        if word in positive_words:\n",
    "            pos_score += 1\n",
    "        elif word in negative_words:\n",
    "            neg_score += 1\n",
    "    return float(pos_score - neg_score)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "sentiment_score_udf = F.udf(sentiment_score, FloatType())\n",
    "\n",
    "reviews_with_sentiment = reviews \\\n",
    "  .withColumn(\n",
    "    'sentiment_score',\n",
    "    sentiment_score_udf(reviews.comments)\n",
    "  )\n",
    "# reviews_with_sentiment.select('comments', 'sentiment_score').show(5, truncate=True)\n",
    "\n",
    "# TODO: Create a final DataFrame\n",
    "final_df = listings.join(reviews_with_sentiment, listings.id == reviews_with_sentiment.listing_id, how=\"inner\")\n",
    "final_df\\\n",
    "    .groupBy(\"listing_id\", \"name\")\\\n",
    "    .agg(\n",
    "        F.avg(\"sentiment_score\").alias('sentiment_avg')\n",
    "    )\\\n",
    "    .orderBy('sentiment_avg', ascending=False)\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9920df16-c74e-450e-9329-3e1015b3c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/19 13:15:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------------------+--------------+\n",
      "|                id|                name|average_comment_length|num_of_reviews|\n",
      "+------------------+--------------------+----------------------+--------------+\n",
      "|618608352812465378|Beautiful Georgia...|    1300.1666666666667|             6|\n",
      "|          28508447|The warm and cosy...|    1089.3333333333333|             6|\n",
      "|627425975703032358|Superb loft beaut...|     951.7777777777778|             9|\n",
      "|           2197681|Luxurious apartme...|                 939.2|             5|\n",
      "|          13891813|Beautiful 2 Bedro...|                 905.0|             5|\n",
      "+------------------+--------------------+----------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 3. Rewrite the following code from the previous exercise using SparkSQL:\n",
    "#\n",
    "# ```\n",
    "# from pyspark.sql.functions import length, avg, count\n",
    "# \n",
    "# reviews_with_comment_length = reviews.withColumn('comment_length', length('comments'))\n",
    "# reviews_with_comment_length \\\n",
    "#   .join(listings, reviews_with_comment_length.listing_id == listings.id, 'inner') \\\n",
    "#   .groupBy('listing_id').agg(\n",
    "#       avg(reviews_with_comment_length.comment_length).alias('average_comment_length'),\n",
    "#       count(reviews_with_comment_length.id).alias('reviews_count')\n",
    "#   ) \\\n",
    "#   .filter('reviews_count >= 5') \\\n",
    "#   .orderBy('average_comment_length', ascending=False) \\\n",
    "#   .show()\n",
    "# ```\n",
    "# This was a solution for the the task:\n",
    "#\n",
    "# \"Get top five listings with the highest average review comment length. Only return listings with at least 5 reviews\"\n",
    "\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "listings.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    listings.id,\n",
    "    listings.name,\n",
    "    AVG(LENGTH(reviews.comments)) AS average_comment_length,\n",
    "    COUNT(reviews.comments) AS num_of_reviews\n",
    "FROM\n",
    "    listings\n",
    "INNER JOIN\n",
    "    reviews\n",
    "ON\n",
    "    listings.id = reviews.listing_id\n",
    "GROUP BY\n",
    "    listings.id, listings.name\n",
    "HAVING\n",
    "    COUNT(reviews.comments) >= 5\n",
    "ORDER BY\n",
    "    average_comment_length DESC  \n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "spark \\\n",
    "  .sql(sql_query) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43846922-68e6-4211-b401-e80036673ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------+\n",
      "|host_id|average_days_since_first_review_days|\n",
      "+-------+------------------------------------+\n",
      "|   6774|                  2025.1666666666667|\n",
      "|   9089|                               508.0|\n",
      "|   9323|                              3018.0|\n",
      "|  10657|                              2185.0|\n",
      "|  11431|                              3614.0|\n",
      "|  14596|                              2701.0|\n",
      "|  19195|                              3665.0|\n",
      "|  25235|                              3301.0|\n",
      "|  26258|                                73.0|\n",
      "|  30577|                               992.0|\n",
      "|  30780|                              3302.0|\n",
      "|  32851|                              3535.5|\n",
      "|  34007|                               370.0|\n",
      "|  36808|                               772.0|\n",
      "|  38691|                               999.0|\n",
      "|  40515|                              2261.0|\n",
      "|  40944|                   553.0833333333334|\n",
      "|  41759|                              5391.0|\n",
      "|  43039|                              4840.0|\n",
      "|  46014|                              2133.0|\n",
      "+-------+------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>                               \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/4.0.0/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# 4. [Optional][Challenge]\n",
    "# Calculate an average time passed from the first review for each host in the listings dataset. \n",
    "# To implmenet a custom aggregation function you would need to use \"pandas_udf\" function to write a custom aggregation function.\n",
    "#\n",
    "# Documentation about \"pandas_udf\": https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html \n",
    "#\n",
    "# To use \"pandas_udf\" you would need to install two additional dependencies in the virtual environment you use for PySpark:\n",
    "# Run these commands:\n",
    "# ```\n",
    "# pip install pandas\n",
    "# pip install pyarrow\n",
    "# ```\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def average_days_since_first_review_udf(first_review_series) -> float:\n",
    "    # TODO: Implement the UDF\n",
    "    today = pd.to_datetime('today')\n",
    "    listings_ages = (today - pd.to_datetime(first_review_series)).dt.days\n",
    "    if listings_ages.empty:\n",
    "        return None\n",
    "    return listings_ages.mean()\n",
    "\n",
    "listings \\\n",
    "  .filter(\n",
    "    listings.first_review.isNotNull()\n",
    "  ) \\\n",
    "  .groupBy('host_id') \\\n",
    "  .agg(\n",
    "    average_days_since_first_review_udf(listings.first_review).alias('average_days_since_first_review_days')\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6256d3d-86b0-4986-9269-8de9957dd959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
