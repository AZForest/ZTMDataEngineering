{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238d41f7-8998-4bf2-a900-e228c7510499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/31 17:19:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Exercise for price estimation\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184ead54-cc6d-4d96-97b8-f588e4ba6f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- sqft_living15: integer (nullable = true)\n",
      " |-- sqft_lot15: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"data/kc_house_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee10cfb3-785a-44ab-b6dd-5cee50dfae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+---+----+-------------+----------+\n",
      "| id|date|price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|lat|long|sqft_living15|sqft_lot15|\n",
      "+---+----+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+---+----+-------------+----------+\n",
      "+---+----+-----+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+---+----+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# u = data.select(F.when(F.col(data.yr_built)).isNull())\n",
    "# u.show()\n",
    "data.filter(data.id.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef6e42d-ade3-4b61-bff0-9c4ddb3d40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+------------+-------+-----+-----+-------------+----------+\n",
      "|   id| date|price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront| view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|  lat| long|sqft_living15|sqft_lot15|\n",
      "+-----+-----+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+------------+-------+-----+-----+-------------+----------+\n",
      "|21613|21613|21613|   21613|    21613|      21613|   21613| 21613|     21613|21613|    21613|21613|     21613|        21613|   21613|       21613|  21613|21613|21613|        21613|     21613|\n",
      "+-----+-----+-----+--------+---------+-----------+--------+------+----------+-----+---------+-----+----------+-------------+--------+------------+-------+-----+-----+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data \\\n",
    "    .select([F.count(F.when(F.col(c).isNotNull(), 1)).alias(c) for c in data.columns]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e6ff6a7-d309-498e-ae70-3420043e6acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  17349\n",
      "Test size:  4264\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Train size: \", train_data.count())\n",
    "print(\"Test size: \", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c6a010a-fb47-435f-a17a-9ec97fbf47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "|        id|           date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "|7129300520|20141013T000000|221900.0|       3|      1.0|       1180|    5650|   1.0|         0|   0|        3|    7|      1180|            0|    1955|           0|  98178|47.5112|-122.257|         1340|      5650|\n",
      "|6414100192|20141209T000000|538000.0|       3|     2.25|       2570|    7242|   2.0|         0|   0|        3|    7|      2170|          400|    1951|        1991|  98125| 47.721|-122.319|         1690|      7639|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "data.select(\"*\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "694276dd-15f2-4fae-9a6c-1b3a4ba1c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|unscaled_features                                                                  |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[6.0,3.0,2400.0,9373.0,2.0,0.0,0.0,3.0,7.0,2400.0,0.0,1991.0,0.0,2060.0,7316.0]    |\n",
      "|[6.0,3.0,2400.0,9373.0,2.0,0.0,0.0,3.0,7.0,2400.0,0.0,1991.0,0.0,2060.0,7316.0]    |\n",
      "|[3.0,1.0,1460.0,43000.0,1.0,0.0,0.0,3.0,7.0,1460.0,0.0,1952.0,0.0,2250.0,20023.0]  |\n",
      "|[3.0,1.0,1430.0,7599.0,1.5,0.0,0.0,4.0,6.0,1010.0,420.0,1930.0,0.0,1290.0,10320.0] |\n",
      "|[4.0,2.0,1650.0,3504.0,1.0,0.0,0.0,3.0,7.0,760.0,890.0,1951.0,2013.0,1480.0,3504.0]|\n",
      "|[5.0,1.5,1990.0,18200.0,1.0,0.0,0.0,3.0,7.0,1990.0,0.0,1960.0,0.0,1860.0,8658.0]   |\n",
      "|[3.0,1.0,1340.0,21336.0,1.5,0.0,0.0,4.0,5.0,1340.0,0.0,1945.0,0.0,1340.0,37703.0]  |\n",
      "|[4.0,2.0,1980.0,10585.0,1.5,0.0,0.0,2.0,6.0,1980.0,0.0,1924.0,0.0,1360.0,7810.0]   |\n",
      "|[2.0,1.0,840.0,12750.0,1.0,0.0,0.0,3.0,6.0,840.0,0.0,1925.0,0.0,1480.0,6969.0]     |\n",
      "|[2.0,1.0,840.0,12750.0,1.0,0.0,0.0,3.0,6.0,840.0,0.0,1925.0,0.0,1480.0,6969.0]     |\n",
      "|[3.0,2.0,1410.0,2700.0,2.0,0.0,0.0,4.0,7.0,1410.0,0.0,1902.0,0.0,1750.0,4000.0]    |\n",
      "|[3.0,2.25,1530.0,1245.0,2.0,0.0,0.0,3.0,9.0,1050.0,480.0,2014.0,0.0,1530.0,2307.0] |\n",
      "|[5.0,1.0,3020.0,4800.0,2.0,0.0,0.0,3.0,7.0,3020.0,0.0,1901.0,0.0,1350.0,1307.0]    |\n",
      "|[2.0,2.0,1130.0,1148.0,2.0,0.0,0.0,3.0,9.0,800.0,330.0,2007.0,0.0,1350.0,1201.0]   |\n",
      "|[2.0,1.0,1420.0,4635.0,2.0,0.0,0.0,4.0,7.0,1420.0,0.0,1941.0,1973.0,1810.0,4635.0] |\n",
      "|[3.0,2.5,1530.0,3464.0,2.0,0.0,0.0,3.0,8.0,1530.0,0.0,1998.0,0.0,1530.0,3446.0]    |\n",
      "|[3.0,2.5,1910.0,4488.0,2.0,0.0,0.0,3.0,8.0,1910.0,0.0,1998.0,0.0,1530.0,3696.0]    |\n",
      "|[3.0,2.5,3350.0,4007.0,2.0,0.0,0.0,3.0,8.0,2550.0,800.0,2005.0,0.0,2340.0,4167.0]  |\n",
      "|[3.0,2.5,3130.0,8750.0,2.0,0.0,0.0,3.0,10.0,3130.0,0.0,1991.0,0.0,2860.0,9003.0]   |\n",
      "|[5.0,2.75,3010.0,12611.0,2.0,0.0,0.0,3.0,10.0,3010.0,0.0,1994.0,0.0,2890.0,9456.0] |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', \\\n",
    "                'sqft_lot', 'floors', \\\n",
    "                'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15' ]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='unscaled_features')\n",
    "train_data = assembler.transform(train_data)\n",
    "train_data.select(\"unscaled_features\").show(truncate=False)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d8a889d-2f1a-4dee-8ef1-c6271a830f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[2.81501044818592...|\n",
      "|[2.81501044818592...|\n",
      "|[-0.3989773725645...|\n",
      "|[-0.3989773725645...|\n",
      "|[0.67235190101892...|\n",
      "|[1.74368117460242...|\n",
      "|[-0.3989773725645...|\n",
      "|[0.67235190101892...|\n",
      "|[-1.4703066461480...|\n",
      "|[-1.4703066461480...|\n",
      "|[-0.3989773725645...|\n",
      "|[-0.3989773725645...|\n",
      "|[1.74368117460242...|\n",
      "|[-1.4703066461480...|\n",
      "|[-1.4703066461480...|\n",
      "|[-0.3989773725645...|\n",
      "|[-0.3989773725645...|\n",
      "|[-0.3989773725645...|\n",
      "|[-0.3989773725645...|\n",
      "|[1.74368117460242...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='unscaled_features', outputCol='features', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_data)\n",
    "transformed_train_data = scaler_model.transform(train_data)\n",
    "transformed_train_data.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91bfd541-7692-4589-8f61-514db39985e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/31 18:11:03 WARN Instrumentation: [0505a8c3] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price')\n",
    "\n",
    "model = lr.fit(transformed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e67d3e9-bdaa-47e9-8de9-e5d75585f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = assembler.transform(test_data)\n",
    "test_data = scaler_model.transform(test_data)\n",
    "# test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1a9497c-4de1-4616-952a-aca6d85d98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.transform(test_data)\n",
    "# test_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68e74114-0709-4e69-a31f-f67d8f205e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 139509.86010701608\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol='price', predictionCol='prediction', metricName='mae')\n",
    "mae = evaluator_mae.evaluate(test_predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fdb22d1-3464-4b32-8119-cbca7a2a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all: Mean Absolute Error (MAE): 139509.86010701608\n",
    "# Without NN: Mean Absolute Error (MAE): 140029.72305802908\n",
    "# Without Enum Cols: Mean Absolute Error (MAE): 161911.9106498095\n",
    "# Note: Same Score with/without StandardScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
